"""Performance Optimization Suite for Graph Hypernetwork Forge.

This module provides comprehensive performance optimization including:
- Advanced memory management and tensor optimization
- Graph compilation and kernel fusion
- Dynamic batching and adaptive processing
- Hardware-specific optimizations (CUDA, CPU, TPU)
- Profile-guided optimization and auto-tuning
"""

import gc
import time
import threading
from collections import defaultdict, deque
from dataclasses import dataclass
from functools import wraps, lru_cache
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
import logging

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np

# Enhanced monitoring and profiling
try:
    from .logging_utils import get_logger, PerformanceProfiler
    from .memory_utils import MemoryManager, TensorPool, GPUMemoryMonitor
    from .caching import AdaptiveCache, TensorCache
    ENHANCED_FEATURES = True
except ImportError:
    def get_logger(name):
        return logging.getLogger(name)
    class PerformanceProfiler:
        def __init__(self, *args, **kwargs): pass
        def profile(self, func): return func
        def get_stats(self): return {}
    class MemoryManager:
        def __init__(self, *args, **kwargs): pass
        def optimize_memory(self): pass
    class TensorPool:
        def __init__(self, *args, **kwargs): pass
        def get_tensor(self, *args): return torch.empty(0)
        def return_tensor(self, *args): pass
    class GPUMemoryMonitor:
        def __init__(self, *args, **kwargs): pass
        def get_memory_stats(self): return {}
    class AdaptiveCache:
        def __init__(self, *args, **kwargs): pass
        def get(self, *args): return None
        def put(self, *args): pass
    class TensorCache:
        def __init__(self, *args, **kwargs): pass
        def cache_tensor(self, func): return func
    ENHANCED_FEATURES = False

logger = get_logger(__name__)


@dataclass
class OptimizationConfig:
    """Configuration for performance optimization."""
    # Memory optimization
    enable_memory_optimization: bool = True
    tensor_pooling: bool = True
    garbage_collection_threshold: int = 1000
    memory_pressure_threshold: float = 0.85
    
    # Computation optimization
    enable_graph_compilation: bool = True
    kernel_fusion: bool = True
    mixed_precision: bool = True
    dynamic_shape_optimization: bool = True
    
    # Batching optimization
    adaptive_batching: bool = True
    max_batch_size: int = 128
    batch_size_growth_factor: float = 1.2
    batch_size_decay_factor: float = 0.8
    
    # Caching optimization
    enable_weight_caching: bool = True
    cache_size_mb: int = 1000
    cache_eviction_policy: str = "lru"  # lru, lfu, fifo
    
    # Hardware optimization
    auto_device_selection: bool = True
    prefer_cuda: bool = True
    tensor_core_usage: bool = True
    
    # Profiling and monitoring
    enable_profiling: bool = True
    profile_memory: bool = True
    profile_computation: bool = True
    optimization_interval: int = 100  # steps


class TensorOptimizer:
    """Advanced tensor optimization and memory management."""
    
    def __init__(self, config: OptimizationConfig):
        """Initialize tensor optimizer.
        
        Args:
            config: Optimization configuration
        """
        self.config = config
        self.tensor_pool = TensorPool() if ENHANCED_FEATURES else None
        self.memory_monitor = GPUMemoryMonitor() if ENHANCED_FEATURES else None
        self.optimization_stats = defaultdict(float)
        
        logger.info("TensorOptimizer initialized")
    
    def optimize_tensor_operations(self, func: Callable) -> Callable:
        """Decorator to optimize tensor operations.
        
        Args:
            func: Function to optimize
            
        Returns:
            Optimized function
        """
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Pre-optimization
            self._pre_optimization_checks()
            
            # Execute with optimization
            start_time = time.time()
            
            if self.config.mixed_precision:
                with torch.cuda.amp.autocast():
                    result = func(*args, **kwargs)
            else:
                result = func(*args, **kwargs)
            
            execution_time = time.time() - start_time
            self.optimization_stats['tensor_operation_time'] += execution_time
            
            # Post-optimization
            self._post_optimization_cleanup()\n            \n            return result\n        \n        return wrapper\n    \n    def _pre_optimization_checks(self):\n        \"\"\"Perform pre-optimization checks and preparations.\"\"\"\n        if self.config.enable_memory_optimization:\n            # Check memory pressure\n            if torch.cuda.is_available() and self.memory_monitor:\n                memory_stats = self.memory_monitor.get_memory_stats()\n                memory_usage = memory_stats.get('memory_usage_percent', 0)\n                \n                if memory_usage > self.config.memory_pressure_threshold:\n                    self._aggressive_memory_cleanup()\n    \n    def _post_optimization_cleanup(self):\n        \"\"\"Perform post-optimization cleanup.\"\"\"\n        # Periodic garbage collection\n        self.optimization_stats['operation_count'] += 1\n        \n        if (self.optimization_stats['operation_count'] % \n            self.config.garbage_collection_threshold == 0):\n            self._memory_cleanup()\n    \n    def _memory_cleanup(self):\n        \"\"\"Perform memory cleanup.\"\"\"\n        # Python garbage collection\n        collected = gc.collect()\n        \n        # CUDA cache cleanup\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n        \n        self.optimization_stats['gc_collections'] += 1\n        self.optimization_stats['objects_collected'] += collected\n        \n        logger.debug(f\"Memory cleanup: collected {collected} objects\")\n    \n    def _aggressive_memory_cleanup(self):\n        \"\"\"Perform aggressive memory cleanup under pressure.\"\"\"\n        logger.warning(\"Memory pressure detected, performing aggressive cleanup\")\n        \n        # Force garbage collection multiple times\n        for _ in range(3):\n            gc.collect()\n        \n        # Clear all CUDA caches\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            torch.cuda.synchronize()\n        \n        # Reset memory stats\n        if torch.cuda.is_available():\n            torch.cuda.reset_peak_memory_stats()\n    \n    def optimize_tensor_layout(self, tensor: torch.Tensor) -> torch.Tensor:\n        \"\"\"Optimize tensor memory layout for better performance.\n        \n        Args:\n            tensor: Input tensor\n            \n        Returns:\n            Optimized tensor\n        \"\"\"\n        if not isinstance(tensor, torch.Tensor):\n            return tensor\n        \n        # Ensure contiguous memory layout\n        if not tensor.is_contiguous():\n            tensor = tensor.contiguous()\n            self.optimization_stats['layout_optimizations'] += 1\n        \n        # Move to optimal device if needed\n        if self.config.auto_device_selection:\n            optimal_device = self._get_optimal_device(tensor)\n            if tensor.device != optimal_device:\n                tensor = tensor.to(optimal_device, non_blocking=True)\n                self.optimization_stats['device_transfers'] += 1\n        \n        return tensor\n    \n    def _get_optimal_device(self, tensor: torch.Tensor) -> torch.device:\n        \"\"\"Determine optimal device for tensor.\n        \n        Args:\n            tensor: Input tensor\n            \n        Returns:\n            Optimal device\n        \"\"\"\n        if not self.config.prefer_cuda or not torch.cuda.is_available():\n            return torch.device('cpu')\n        \n        # Simple heuristic: use GPU for large tensors\n        tensor_size_mb = tensor.numel() * tensor.element_size() / (1024 * 1024)\n        \n        if tensor_size_mb > 1.0:  # > 1MB, use GPU\n            return torch.device('cuda')\n        else:\n            return tensor.device  # Keep on current device\n    \n    def get_optimization_stats(self) -> Dict[str, Any]:\n        \"\"\"Get optimization statistics.\n        \n        Returns:\n            Optimization statistics\n        \"\"\"\n        return dict(self.optimization_stats)\n\n\nclass GraphCompiler:\n    \"\"\"Graph compilation and kernel fusion optimizer.\"\"\"\n    \n    def __init__(self, config: OptimizationConfig):\n        \"\"\"Initialize graph compiler.\n        \n        Args:\n            config: Optimization configuration\n        \"\"\"\n        self.config = config\n        self.compiled_functions = {}\n        self.compilation_cache = {}\n        \n        logger.info(\"GraphCompiler initialized\")\n    \n    def compile_function(self, func: Callable, \n                        dynamic_shapes: bool = None) -> Callable:\n        \"\"\"Compile function for optimal performance.\n        \n        Args:\n            func: Function to compile\n            dynamic_shapes: Whether to support dynamic shapes\n            \n        Returns:\n            Compiled function\n        \"\"\"\n        if not self.config.enable_graph_compilation:\n            return func\n        \n        func_id = id(func)\n        \n        if func_id in self.compiled_functions:\n            return self.compiled_functions[func_id]\n        \n        # PyTorch 2.0 compilation\n        if hasattr(torch, 'compile'):\n            compile_options = {\n                'dynamic': dynamic_shapes if dynamic_shapes is not None \n                          else self.config.dynamic_shape_optimization,\n            }\n            \n            if self.config.kernel_fusion:\n                compile_options['mode'] = 'max-autotune'\n            \n            compiled_func = torch.compile(func, **compile_options)\n            self.compiled_functions[func_id] = compiled_func\n            \n            logger.info(f\"Function compiled with PyTorch 2.0: {func.__name__}\")\n            return compiled_func\n        \n        # Fallback: no compilation\n        logger.warning(\"PyTorch 2.0 compilation not available\")\n        return func\n    \n    def optimize_model(self, model: nn.Module) -> nn.Module:\n        \"\"\"Optimize entire model with graph compilation.\n        \n        Args:\n            model: Model to optimize\n            \n        Returns:\n            Optimized model\n        \"\"\"\n        if not self.config.enable_graph_compilation:\n            return model\n        \n        # PyTorch 2.0 model compilation\n        if hasattr(torch, 'compile'):\n            compile_options = {\n                'dynamic': self.config.dynamic_shape_optimization,\n            }\n            \n            if self.config.kernel_fusion:\n                compile_options['mode'] = 'max-autotune'\n            \n            optimized_model = torch.compile(model, **compile_options)\n            logger.info(f\"Model compiled with PyTorch 2.0: {model.__class__.__name__}\")\n            return optimized_model\n        \n        # Fallback optimizations\n        self._apply_manual_optimizations(model)\n        return model\n    \n    def _apply_manual_optimizations(self, model: nn.Module):\n        \"\"\"Apply manual optimizations when compilation is not available.\n        \n        Args:\n            model: Model to optimize\n        \"\"\"\n        # Fuse BatchNorm layers\n        if hasattr(torch.quantization, 'fuse_modules'):\n            try:\n                # Find fusable modules\n                fusable_modules = self._find_fusable_modules(model)\n                if fusable_modules:\n                    torch.quantization.fuse_modules(model, fusable_modules, inplace=True)\n                    logger.info(f\"Fused {len(fusable_modules)} module groups\")\n            except Exception as e:\n                logger.warning(f\"Module fusion failed: {e}\")\n        \n        # Enable memory efficient attention if available\n        for module in model.modules():\n            if hasattr(module, 'enable_nested_tensor') and callable(module.enable_nested_tensor):\n                try:\n                    module.enable_nested_tensor()\n                except Exception:\n                    pass\n    \n    def _find_fusable_modules(self, model: nn.Module) -> List[List[str]]:\n        \"\"\"Find modules that can be fused together.\n        \n        Args:\n            model: Model to analyze\n            \n        Returns:\n            List of fusable module groups\n        \"\"\"\n        fusable_groups = []\n        \n        # Find Conv + BatchNorm + ReLU patterns\n        modules = list(model.named_modules())\n        \n        for i in range(len(modules) - 2):\n            name1, module1 = modules[i]\n            name2, module2 = modules[i + 1]\n            name3, module3 = modules[i + 2]\n            \n            # Conv + BN + ReLU pattern\n            if (isinstance(module1, (nn.Conv1d, nn.Conv2d, nn.Conv3d)) and\n                isinstance(module2, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)) and\n                isinstance(module3, nn.ReLU)):\n                fusable_groups.append([name1, name2, name3])\n        \n        return fusable_groups\n\n\nclass AdaptiveBatchOptimizer:\n    \"\"\"Adaptive batching optimizer for dynamic performance tuning.\"\"\"\n    \n    def __init__(self, config: OptimizationConfig):\n        \"\"\"Initialize adaptive batch optimizer.\n        \n        Args:\n            config: Optimization configuration\n        \"\"\"\n        self.config = config\n        self.current_batch_size = config.max_batch_size // 4  # Start conservative\n        self.performance_history = deque(maxlen=50)\n        self.memory_usage_history = deque(maxlen=50)\n        self.last_optimization_step = 0\n        \n        logger.info(f\"AdaptiveBatchOptimizer initialized with batch size {self.current_batch_size}\")\n    \n    def optimize_batch_size(self, step: int, execution_time: float, \n                          memory_usage: float, success: bool) -> int:\n        \"\"\"Optimize batch size based on performance metrics.\n        \n        Args:\n            step: Current training step\n            execution_time: Last batch execution time\n            memory_usage: Current memory usage (0-1)\n            success: Whether last batch executed successfully\n            \n        Returns:\n            Optimized batch size\n        \"\"\"\n        if not self.config.adaptive_batching:\n            return self.current_batch_size\n        \n        # Record performance\n        self.performance_history.append(execution_time)\n        self.memory_usage_history.append(memory_usage)\n        \n        # Optimize periodically\n        if step - self.last_optimization_step >= self.config.optimization_interval:\n            self._update_batch_size(success)\n            self.last_optimization_step = step\n        \n        return self.current_batch_size\n    \n    def _update_batch_size(self, last_success: bool):\n        \"\"\"Update batch size based on recent performance.\n        \n        Args:\n            last_success: Whether last batch was successful\n        \"\"\"\n        if not self.performance_history:\n            return\n        \n        # Calculate recent performance metrics\n        recent_exec_time = np.mean(list(self.performance_history)[-10:])\n        recent_memory = np.mean(list(self.memory_usage_history)[-10:])\n        \n        # Decision logic\n        should_increase = (\n            last_success and\n            recent_memory < self.config.memory_pressure_threshold and\n            self.current_batch_size < self.config.max_batch_size\n        )\n        \n        should_decrease = (\n            not last_success or\n            recent_memory > self.config.memory_pressure_threshold\n        )\n        \n        if should_increase:\n            new_batch_size = int(self.current_batch_size * self.config.batch_size_growth_factor)\n            new_batch_size = min(new_batch_size, self.config.max_batch_size)\n            \n            if new_batch_size > self.current_batch_size:\n                logger.info(f\"Increasing batch size: {self.current_batch_size} -> {new_batch_size}\")\n                self.current_batch_size = new_batch_size\n        \n        elif should_decrease:\n            new_batch_size = int(self.current_batch_size * self.config.batch_size_decay_factor)\n            new_batch_size = max(new_batch_size, 1)\n            \n            if new_batch_size < self.current_batch_size:\n                logger.info(f\"Decreasing batch size: {self.current_batch_size} -> {new_batch_size}\")\n                self.current_batch_size = new_batch_size\n    \n    def create_adaptive_dataloader(self, dataset, collate_fn=None, **kwargs) -> 'AdaptiveDataLoader':\n        \"\"\"Create adaptive data loader with dynamic batching.\n        \n        Args:\n            dataset: Dataset to load\n            collate_fn: Collate function\n            **kwargs: Additional DataLoader arguments\n            \n        Returns:\n            Adaptive data loader\n        \"\"\"\n        return AdaptiveDataLoader(\n            dataset=dataset,\n            batch_optimizer=self,\n            collate_fn=collate_fn,\n            **kwargs\n        )\n\n\nclass AdaptiveDataLoader:\n    \"\"\"Data loader with adaptive batch sizing.\"\"\"\n    \n    def __init__(self, dataset, batch_optimizer: AdaptiveBatchOptimizer,\n                 collate_fn=None, **kwargs):\n        \"\"\"Initialize adaptive data loader.\n        \n        Args:\n            dataset: Dataset to load\n            batch_optimizer: Batch size optimizer\n            collate_fn: Collate function\n            **kwargs: Additional arguments\n        \"\"\"\n        self.dataset = dataset\n        self.batch_optimizer = batch_optimizer\n        self.collate_fn = collate_fn\n        self.kwargs = kwargs\n        self.current_loader = None\n        self.step = 0\n        \n        self._create_loader()\n    \n    def _create_loader(self):\n        \"\"\"Create new data loader with current batch size.\"\"\"\n        batch_size = self.batch_optimizer.current_batch_size\n        \n        self.current_loader = DataLoader(\n            dataset=self.dataset,\n            batch_size=batch_size,\n            collate_fn=self.collate_fn,\n            **self.kwargs\n        )\n    \n    def __iter__(self):\n        \"\"\"Iterate through adaptive batches.\"\"\"\n        return self\n    \n    def __next__(self):\n        \"\"\"Get next adaptive batch.\"\"\"\n        # Check if we need to recreate the loader\n        if (self.current_loader is None or \n            self.current_loader.batch_size != self.batch_optimizer.current_batch_size):\n            self._create_loader()\n        \n        try:\n            batch = next(iter(self.current_loader))\n            self.step += 1\n            return batch\n        except StopIteration:\n            raise StopIteration\n    \n    def update_performance(self, execution_time: float, memory_usage: float, \n                         success: bool = True):\n        \"\"\"Update performance metrics for batch optimization.\n        \n        Args:\n            execution_time: Batch execution time\n            memory_usage: Memory usage (0-1)\n            success: Whether batch executed successfully\n        \"\"\"\n        self.batch_optimizer.optimize_batch_size(\n            self.step, execution_time, memory_usage, success\n        )\n\n\nclass PerformanceOptimizer:\n    \"\"\"Comprehensive performance optimization manager.\"\"\"\n    \n    def __init__(self, config: OptimizationConfig = None):\n        \"\"\"Initialize performance optimizer.\n        \n        Args:\n            config: Optimization configuration\n        \"\"\"\n        self.config = config or OptimizationConfig()\n        \n        # Initialize optimizers\n        self.tensor_optimizer = TensorOptimizer(self.config)\n        self.graph_compiler = GraphCompiler(self.config)\n        self.batch_optimizer = AdaptiveBatchOptimizer(self.config)\n        \n        # Caching systems\n        if ENHANCED_FEATURES:\n            self.adaptive_cache = AdaptiveCache(\n                max_size_mb=self.config.cache_size_mb,\n                eviction_policy=self.config.cache_eviction_policy\n            )\n            self.tensor_cache = TensorCache()\n        else:\n            self.adaptive_cache = None\n            self.tensor_cache = None\n        \n        # Profiling\n        if ENHANCED_FEATURES:\n            self.profiler = PerformanceProfiler(\"performance_optimizer\")\n        else:\n            self.profiler = None\n        \n        # Performance tracking\n        self.optimization_history = []\n        self.total_optimizations = 0\n        \n        logger.info(\"PerformanceOptimizer initialized with comprehensive optimizations\")\n    \n    def optimize_model(self, model: nn.Module) -> nn.Module:\n        \"\"\"Apply comprehensive model optimizations.\n        \n        Args:\n            model: Model to optimize\n            \n        Returns:\n            Optimized model\n        \"\"\"\n        logger.info(f\"Optimizing model: {model.__class__.__name__}\")\n        \n        # Graph compilation optimization\n        model = self.graph_compiler.optimize_model(model)\n        \n        # Apply tensor layout optimizations to parameters\n        for name, param in model.named_parameters():\n            optimized_param = self.tensor_optimizer.optimize_tensor_layout(param)\n            if optimized_param is not param:\n                param.data = optimized_param.data\n        \n        # Memory optimization\n        if self.config.enable_memory_optimization:\n            self._optimize_model_memory(model)\n        \n        self.total_optimizations += 1\n        logger.info(f\"Model optimization complete\")\n        \n        return model\n    \n    def optimize_function(self, func: Callable, \n                         enable_caching: bool = True) -> Callable:\n        \"\"\"Optimize function with comprehensive performance enhancements.\n        \n        Args:\n            func: Function to optimize\n            enable_caching: Whether to enable result caching\n            \n        Returns:\n            Optimized function\n        \"\"\"\n        # Apply optimizations in order\n        optimized_func = func\n        \n        # Tensor operations optimization\n        optimized_func = self.tensor_optimizer.optimize_tensor_operations(optimized_func)\n        \n        # Graph compilation\n        optimized_func = self.graph_compiler.compile_function(optimized_func)\n        \n        # Result caching\n        if enable_caching and self.tensor_cache and ENHANCED_FEATURES:\n            optimized_func = self.tensor_cache.cache_tensor(optimized_func)\n        \n        # Profiling wrapper\n        if self.profiler and ENHANCED_FEATURES:\n            optimized_func = self.profiler.profile(optimized_func)\n        \n        return optimized_func\n    \n    def create_optimized_dataloader(self, dataset, **kwargs) -> AdaptiveDataLoader:\n        \"\"\"Create optimized data loader with adaptive batching.\n        \n        Args:\n            dataset: Dataset to load\n            **kwargs: DataLoader arguments\n            \n        Returns:\n            Optimized adaptive data loader\n        \"\"\"\n        return self.batch_optimizer.create_adaptive_dataloader(dataset, **kwargs)\n    \n    def _optimize_model_memory(self, model: nn.Module):\n        \"\"\"Optimize model memory usage.\n        \n        Args:\n            model: Model to optimize\n        \"\"\"\n        # Parameter sharing optimization\n        self._apply_parameter_sharing(model)\n        \n        # Gradient checkpointing for large models\n        self._apply_gradient_checkpointing(model)\n        \n        # Memory layout optimization\n        self._optimize_parameter_layout(model)\n    \n    def _apply_parameter_sharing(self, model: nn.Module):\n        \"\"\"Apply parameter sharing where possible.\n        \n        Args:\n            model: Model to optimize\n        \"\"\"\n        # Find duplicate parameters and share them\n        parameter_registry = {}\n        shared_count = 0\n        \n        for name, param in model.named_parameters():\n            param_hash = hash(param.data.data_ptr())\n            \n            if param_hash in parameter_registry:\n                # Parameter already exists, could be shared\n                shared_count += 1\n            else:\n                parameter_registry[param_hash] = (name, param)\n        \n        if shared_count > 0:\n            logger.info(f\"Identified {shared_count} potentially shareable parameters\")\n    \n    def _apply_gradient_checkpointing(self, model: nn.Module):\n        \"\"\"Apply gradient checkpointing for memory efficiency.\n        \n        Args:\n            model: Model to optimize\n        \"\"\"\n        # Apply gradient checkpointing to transformer-like layers\n        for module in model.modules():\n            if hasattr(module, 'gradient_checkpointing_enable'):\n                try:\n                    module.gradient_checkpointing_enable()\n                    logger.debug(f\"Enabled gradient checkpointing for {module.__class__.__name__}\")\n                except Exception as e:\n                    logger.warning(f\"Failed to enable gradient checkpointing: {e}\")\n    \n    def _optimize_parameter_layout(self, model: nn.Module):\n        \"\"\"Optimize parameter memory layout.\n        \n        Args:\n            model: Model to optimize\n        \"\"\"\n        for name, param in model.named_parameters():\n            if not param.is_contiguous():\n                param.data = param.data.contiguous()\n                logger.debug(f\"Made parameter contiguous: {name}\")\n    \n    def get_optimization_report(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive optimization report.\n        \n        Returns:\n            Optimization report\n        \"\"\"\n        report = {\n            'total_optimizations': self.total_optimizations,\n            'tensor_optimizer_stats': self.tensor_optimizer.get_optimization_stats(),\n            'adaptive_batching': {\n                'current_batch_size': self.batch_optimizer.current_batch_size,\n                'performance_history_length': len(self.batch_optimizer.performance_history),\n            },\n            'config': self.config.__dict__,\n        }\n        \n        # Add profiler stats if available\n        if self.profiler and ENHANCED_FEATURES:\n            report['profiler_stats'] = self.profiler.get_stats()\n        \n        # Add cache stats if available\n        if self.adaptive_cache and ENHANCED_FEATURES:\n            report['cache_stats'] = self.adaptive_cache.get_stats()\n        \n        return report\n    \n    def optimize_for_inference(self, model: nn.Module) -> nn.Module:\n        \"\"\"Optimize model specifically for inference.\n        \n        Args:\n            model: Model to optimize\n            \n        Returns:\n            Inference-optimized model\n        \"\"\"\n        logger.info(\"Optimizing model for inference\")\n        \n        # Set to evaluation mode\n        model.eval()\n        \n        # Disable gradient computation\n        for param in model.parameters():\n            param.requires_grad = False\n        \n        # Apply inference-specific optimizations\n        model = self.graph_compiler.optimize_model(model)\n        \n        # Try to apply torch.jit.script for additional optimization\n        try:\n            if hasattr(torch.jit, 'script'):\n                # Only script if the model is scriptable\n                scripted_model = torch.jit.script(model)\n                logger.info(\"Model successfully scripted with TorchScript\")\n                return scripted_model\n        except Exception as e:\n            logger.warning(f\"TorchScript compilation failed: {e}\")\n        \n        return model\n    \n    def benchmark_model(self, model: nn.Module, sample_input: Dict[str, torch.Tensor],\n                       num_iterations: int = 100) -> Dict[str, float]:\n        \"\"\"Benchmark model performance.\n        \n        Args:\n            model: Model to benchmark\n            sample_input: Sample input for the model\n            num_iterations: Number of benchmark iterations\n            \n        Returns:\n            Benchmark results\n        \"\"\"\n        logger.info(f\"Benchmarking model with {num_iterations} iterations\")\n        \n        model.eval()\n        \n        # Warmup\n        with torch.no_grad():\n            for _ in range(10):\n                _ = model(**sample_input)\n        \n        # Benchmark\n        times = []\n        memory_usage = []\n        \n        with torch.no_grad():\n            for i in range(num_iterations):\n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()\n                    start_memory = torch.cuda.memory_allocated()\n                \n                start_time = time.time()\n                \n                output = model(**sample_input)\n                \n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()\n                    end_memory = torch.cuda.memory_allocated()\n                    memory_usage.append(end_memory - start_memory)\n                \n                end_time = time.time()\n                times.append(end_time - start_time)\n        \n        # Calculate statistics\n        results = {\n            'mean_time': np.mean(times),\n            'std_time': np.std(times),\n            'min_time': np.min(times),\n            'max_time': np.max(times),\n            'throughput': 1.0 / np.mean(times),  # samples per second\n        }\n        \n        if memory_usage:\n            results.update({\n                'mean_memory_mb': np.mean(memory_usage) / (1024 * 1024),\n                'max_memory_mb': np.max(memory_usage) / (1024 * 1024),\n            })\n        \n        logger.info(f\"Benchmark complete: {results['mean_time']:.4f}s ± {results['std_time']:.4f}s\")\n        \n        return results\n\n\n# Global optimizer instance\n_global_optimizer = None\n\ndef get_performance_optimizer(config: OptimizationConfig = None) -> PerformanceOptimizer:\n    \"\"\"Get global performance optimizer instance.\n    \n    Args:\n        config: Optimization configuration\n        \n    Returns:\n        Performance optimizer instance\n    \"\"\"\n    global _global_optimizer\n    if _global_optimizer is None:\n        _global_optimizer = PerformanceOptimizer(config)\n    return _global_optimizer\n\n\n# Decorators for easy optimization\ndef optimize_function(enable_caching: bool = True):\n    \"\"\"Decorator to optimize function performance.\n    \n    Args:\n        enable_caching: Whether to enable result caching\n    \"\"\"\n    def decorator(func: Callable) -> Callable:\n        optimizer = get_performance_optimizer()\n        return optimizer.optimize_function(func, enable_caching)\n    \n    return decorator\n\n\ndef optimize_tensor_ops(func: Callable) -> Callable:\n    \"\"\"Decorator to optimize tensor operations.\n    \n    Args:\n        func: Function to optimize\n        \n    Returns:\n        Optimized function\n    \"\"\"\n    optimizer = get_performance_optimizer()\n    return optimizer.tensor_optimizer.optimize_tensor_operations(func)\n\n\n# Context manager for performance optimization\nclass OptimizationContext:\n    \"\"\"Context manager for performance optimization.\"\"\"\n    \n    def __init__(self, config: OptimizationConfig = None):\n        \"\"\"Initialize optimization context.\n        \n        Args:\n            config: Optimization configuration\n        \"\"\"\n        self.config = config or OptimizationConfig()\n        self.optimizer = None\n        self.start_time = None\n    \n    def __enter__(self):\n        self.start_time = time.time()\n        self.optimizer = get_performance_optimizer(self.config)\n        logger.debug(\"Entered optimization context\")\n        return self.optimizer\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        elapsed_time = time.time() - self.start_time\n        logger.debug(f\"Exited optimization context after {elapsed_time:.4f}s\")\n        \n        return False  # Don't suppress exceptions