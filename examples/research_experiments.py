"""Advanced Research Experiments and Comparative Studies.

This module demonstrates cutting-edge research experiments including:
- Comparative analysis of hypernetwork vs traditional approaches
- Novel algorithm validation with statistical significance testing
- Performance benchmarking across multiple domains
- Ablation studies of hypernetwork components
"""

import json
import time
import warnings
from typing import Dict, List, Tuple, Any
import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

import torch
import torch.nn.functional as F
from torch_geometric.data import Data, DataLoader
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support
from sklearn.model_selection import train_test_split

# Import our models
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from graph_hypernetwork_forge.models.hypergnn import HyperGNN
from graph_hypernetwork_forge.models.advanced_hypernetworks import (
    DimensionAdaptiveHyperNetwork, 
    TransformerWeightGenerator,
    AttentionWeightGenerator,
    MetaLearningHyperNetwork
)
from graph_hypernetwork_forge.models.research_baselines import (
    TraditionalGNN,
    MAMLBaseline, 
    PrototypicalNetworkBaseline,
    FineTuningBaseline,
    ZeroShotTextGNN
)
from graph_hypernetwork_forge.utils.datasets import SyntheticDataGenerator
from graph_hypernetwork_forge.data.knowledge_graph import TextualKnowledgeGraph

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')


class ResearchExperimentSuite:
    """Comprehensive research experiment suite for HyperGNN evaluation."""
    
    def __init__(self, device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        """Initialize research experiment suite.
        
        Args:
            device: Device for experiments (cuda or cpu)
        """
        self.device = device
        self.results = {}
        self.experiment_history = []
        
        print(f"üî¨ Research Experiment Suite initialized on {device}")
        print("=" * 60)
    
    def run_comprehensive_benchmark(
        self,
        domains: List[str] = None,
        num_graphs_per_domain: int = 5,
        num_runs: int = 3,
        significance_level: float = 0.05,
    ) -> Dict[str, Any]:
        """Run comprehensive benchmark across multiple domains and methods.
        
        Args:
            domains: List of domains to test
            num_graphs_per_domain: Number of graphs per domain
            num_runs: Number of statistical runs
            significance_level: Statistical significance threshold
            
        Returns:
            Comprehensive results dictionary
        """
        if domains is None:
            domains = ["social", "citation", "biological", "financial", "product"]
        
        print(f"üöÄ Running Comprehensive Benchmark")
        print(f"üìä Domains: {domains}")
        print(f"üìà Graphs per domain: {num_graphs_per_domain}")
        print(f"üîÑ Statistical runs: {num_runs}")
        print("-" * 60)
        
        # Generate datasets
        print("üìä Generating datasets...")
        datasets = self._generate_multi_domain_datasets(domains, num_graphs_per_domain)
        
        # Initialize models
        print("ü§ñ Initializing models...")
        models = self._initialize_all_models()
        
        # Run experiments
        all_results = {}
        
        for run in range(num_runs):
            print(f"\nüîÑ Statistical Run {run + 1}/{num_runs}")
            print("-" * 40)
            
            run_results = {}
            
            for model_name, model in models.items():
                print(f"üß† Testing {model_name}...")
                
                model_results = self._evaluate_model_on_datasets(
                    model, model_name, datasets
                )
                run_results[model_name] = model_results
            
            all_results[f"run_{run}"] = run_results
        
        # Statistical analysis
        print("\nüìä Performing Statistical Analysis...")
        statistical_results = self._perform_statistical_analysis(
            all_results, significance_level
        )
        
        # Generate comprehensive report
        comprehensive_results = {
            "raw_results": all_results,
            "statistical_analysis": statistical_results,
            "experiment_config": {
                "domains": domains,
                "num_graphs_per_domain": num_graphs_per_domain,
                "num_runs": num_runs,
                "significance_level": significance_level,
                "device": self.device,
            },
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        }
        
        # Save results
        self._save_results(comprehensive_results, "comprehensive_benchmark")
        
        # Generate visualizations
        self._generate_visualizations(comprehensive_results)
        
        print("\n‚úÖ Comprehensive Benchmark Complete!")
        print(f"üìÅ Results saved to benchmark_results/")
        
        return comprehensive_results
    
    def run_ablation_study(self) -> Dict[str, Any]:
        """Run ablation study on hypernetwork components.
        
        Returns:
            Ablation study results
        """
        print("üî¨ Running Ablation Study on HyperGNN Components")
        print("-" * 50)
        
        # Generate test dataset
        data_gen = SyntheticDataGenerator()
        graphs = data_gen.generate_domain_graphs("citation", num_graphs=10)
        
        # Ablation configurations
        ablation_configs = {
            "full_hypergnn": {
                "use_attention": True,
                "use_transformer": False,
                "enable_caching": True,
                "adaptive_dims": True,
            },
            "no_attention": {
                "use_attention": False,
                "use_transformer": False,
                "enable_caching": True,
                "adaptive_dims": True,
            },
            "with_transformer": {
                "use_attention": False,
                "use_transformer": True,
                "enable_caching": True,
                "adaptive_dims": True,
            },
            "no_caching": {
                "use_attention": True,
                "use_transformer": False,
                "enable_caching": False,
                "adaptive_dims": True,
            },
            "fixed_dims": {
                "use_attention": True,
                "use_transformer": False,
                "enable_caching": True,
                "adaptive_dims": False,
            },
        }\n        \n        results = {}\n        \n        for config_name, config in ablation_configs.items():\n            print(f\"üß™ Testing configuration: {config_name}\")\n            \n            # Create model with specific configuration\n            if config[\"adaptive_dims\"]:\n                model = DimensionAdaptiveHyperNetwork(\n                    text_dim=256,\n                    max_hidden_dim=256,\n                    use_attention=config[\"use_attention\"],\n                    use_transformer=config[\"use_transformer\"],\n                ).to(self.device)\n            else:\n                model = HyperGNN(\n                    hidden_dim=256,\n                    enable_caching=config[\"enable_caching\"],\n                ).to(self.device)\n            \n            # Evaluate on test graphs\n            config_results = []\n            \n            for graph in graphs[:5]:  # Use subset for ablation\n                kg = TextualKnowledgeGraph.from_dict(graph)\n                \n                start_time = time.time()\n                try:\n                    with torch.no_grad():\n                        predictions = model(\n                            kg.edge_index.to(self.device),\n                            kg.node_features.to(self.device),\n                            kg.node_texts,\n                        )\n                    \n                    inference_time = time.time() - start_time\n                    success = True\n                    \n                except Exception as e:\n                    print(f\"   ‚ùå Error: {e}\")\n                    inference_time = float('inf')\n                    success = False\n                \n                config_results.append({\n                    \"inference_time\": inference_time,\n                    \"success\": success,\n                    \"num_nodes\": len(kg.node_texts),\n                    \"num_edges\": kg.edge_index.size(1),\n                })\n            \n            # Aggregate results\n            successful_results = [r for r in config_results if r[\"success\"]]\n            \n            if successful_results:\n                avg_time = np.mean([r[\"inference_time\"] for r in successful_results])\n                success_rate = len(successful_results) / len(config_results)\n            else:\n                avg_time = float('inf')\n                success_rate = 0.0\n            \n            results[config_name] = {\n                \"avg_inference_time\": avg_time,\n                \"success_rate\": success_rate,\n                \"config\": config,\n                \"detailed_results\": config_results,\n            }\n            \n            print(f\"   ‚úÖ Avg Time: {avg_time:.4f}s, Success Rate: {success_rate:.2%}\")\n        \n        # Analysis\n        print(\"\\nüìä Ablation Study Analysis:\")\n        print(\"-\" * 30)\n        \n        sorted_configs = sorted(\n            results.items(), \n            key=lambda x: (x[1][\"success_rate\"], -x[1][\"avg_inference_time\"]),\n            reverse=True\n        )\n        \n        for i, (config_name, result) in enumerate(sorted_configs):\n            rank = i + 1\n            print(f\"{rank}. {config_name}: \"\n                  f\"Success={result['success_rate']:.2%}, \"\n                  f\"Time={result['avg_inference_time']:.4f}s\")\n        \n        ablation_results = {\n            \"results\": results,\n            \"ranking\": [name for name, _ in sorted_configs],\n            \"analysis\": self._analyze_ablation_results(results),\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        }\n        \n        self._save_results(ablation_results, \"ablation_study\")\n        \n        return ablation_results\n    \n    def run_zero_shot_transfer_study(self) -> Dict[str, Any]:\n        \"\"\"Run comprehensive zero-shot transfer learning study.\n        \n        Returns:\n            Zero-shot transfer results\n        \"\"\"\n        print(\"üéØ Running Zero-Shot Transfer Learning Study\")\n        print(\"-\" * 45)\n        \n        domains = [\"social\", \"citation\", \"biological\", \"financial\"]\n        \n        # Generate datasets for each domain\n        datasets = {}\n        data_gen = SyntheticDataGenerator()\n        \n        for domain in domains:\n            print(f\"üìä Generating {domain} dataset...\")\n            graphs = data_gen.generate_domain_graphs(domain, num_graphs=8)\n            # Split into source and target\n            datasets[domain] = {\n                \"source\": graphs[:5],\n                \"target\": graphs[5:],\n            }\n        \n        # Initialize models\n        models = {\n            \"HyperGNN\": HyperGNN(hidden_dim=128).to(self.device),\n            \"ZeroShotText\": ZeroShotTextGNN(\n                input_dim=128, hidden_dim=128, output_dim=64\n            ).to(self.device),\n            \"MAML\": MAMLBaseline(\n                input_dim=128, hidden_dim=128, output_dim=64\n            ).to(self.device),\n        }\n        \n        transfer_results = {}\n        \n        # Test all domain pairs\n        for source_domain in domains:\n            for target_domain in domains:\n                if source_domain == target_domain:\n                    continue\n                \n                pair_name = f\"{source_domain}_to_{target_domain}\"\n                print(f\"\\nüîÑ Testing transfer: {pair_name}\")\n                \n                pair_results = {}\n                \n                for model_name, model in models.items():\n                    print(f\"   üß† {model_name}...\")\n                    \n                    try:\n                        # Prepare data\n                        source_graphs = datasets[source_domain][\"source\"]\n                        target_graphs = datasets[target_domain][\"target\"]\n                        \n                        if model_name == \"HyperGNN\":\n                            # Direct zero-shot transfer\n                            transfer_accuracy = self._test_hypergnn_transfer(\n                                model, source_graphs, target_graphs\n                            )\n                        \n                        elif model_name == \"ZeroShotText\":\n                            # Text similarity based transfer\n                            transfer_accuracy = self._test_text_similarity_transfer(\n                                model, source_graphs, target_graphs\n                            )\n                        \n                        elif model_name == \"MAML\":\n                            # Meta-learning based transfer\n                            transfer_accuracy = self._test_maml_transfer(\n                                model, source_graphs, target_graphs\n                            )\n                        \n                        pair_results[model_name] = {\n                            \"accuracy\": transfer_accuracy,\n                            \"success\": True,\n                        }\n                        \n                        print(f\"      ‚úÖ Accuracy: {transfer_accuracy:.3f}\")\n                    \n                    except Exception as e:\n                        print(f\"      ‚ùå Error: {e}\")\n                        pair_results[model_name] = {\n                            \"accuracy\": 0.0,\n                            \"success\": False,\n                            \"error\": str(e),\n                        }\n                \n                transfer_results[pair_name] = pair_results\n        \n        # Analysis\n        print(\"\\nüìä Zero-Shot Transfer Analysis:\")\n        print(\"-\" * 35)\n        \n        # Compute average performance per model\n        model_averages = {}\n        for model_name in models.keys():\n            accuracies = [\n                result[model_name][\"accuracy\"] \n                for result in transfer_results.values()\n                if result[model_name][\"success\"]\n            ]\n            \n            if accuracies:\n                avg_acc = np.mean(accuracies)\n                std_acc = np.std(accuracies)\n                model_averages[model_name] = {\n                    \"mean_accuracy\": avg_acc,\n                    \"std_accuracy\": std_acc,\n                    \"num_successful_transfers\": len(accuracies),\n                }\n                \n                print(f\"{model_name}: {avg_acc:.3f} ¬± {std_acc:.3f} \"\n                      f\"({len(accuracies)}/{len(transfer_results)} successful)\")\n        \n        zero_shot_results = {\n            \"transfer_matrix\": transfer_results,\n            \"model_averages\": model_averages,\n            \"domains\": domains,\n            \"analysis\": self._analyze_transfer_results(transfer_results),\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        }\n        \n        self._save_results(zero_shot_results, \"zero_shot_transfer\")\n        \n        return zero_shot_results\n    \n    def run_scalability_study(self) -> Dict[str, Any]:\n        \"\"\"Run scalability study with varying graph sizes.\n        \n        Returns:\n            Scalability study results\n        \"\"\"\n        print(\"üìà Running Scalability Study\")\n        print(\"-\" * 30)\n        \n        # Test different graph sizes\n        graph_sizes = [10, 50, 100, 200, 500, 1000]\n        models = {\n            \"HyperGNN\": HyperGNN(hidden_dim=128).to(self.device),\n            \"TraditionalGAT\": TraditionalGNN(\n                input_dim=128, hidden_dim=128, output_dim=64, gnn_type=\"GAT\"\n            ).to(self.device),\n        }\n        \n        scalability_results = {}\n        \n        for size in graph_sizes:\n            print(f\"\\nüìä Testing graph size: {size} nodes\")\n            \n            # Generate synthetic graph of specific size\n            data_gen = SyntheticDataGenerator()\n            graph_data = data_gen._create_single_graph(\n                num_nodes=size,\n                domain=\"citation\",\n                edge_prob=0.05,  # Adjust for reasonable edge density\n            )\n            \n            kg = TextualKnowledgeGraph.from_dict(graph_data)\n            \n            size_results = {}\n            \n            for model_name, model in models.items():\n                print(f\"   üß† {model_name}...\")\n                \n                # Measure inference time and memory\n                try:\n                    model.eval()\n                    \n                    # Warm up\n                    with torch.no_grad():\n                        _ = model(\n                            kg.edge_index.to(self.device),\n                            kg.node_features.to(self.device),\n                            kg.node_texts if model_name == \"HyperGNN\" else None,\n                        )\n                    \n                    # Benchmark\n                    times = []\n                    for _ in range(5):  # Multiple runs for accuracy\n                        start_time = time.time()\n                        \n                        with torch.no_grad():\n                            predictions = model(\n                                kg.edge_index.to(self.device),\n                                kg.node_features.to(self.device),\n                                kg.node_texts if model_name == \"HyperGNN\" else None,\n                            )\n                        \n                        torch.cuda.synchronize() if self.device == \"cuda\" else None\n                        inference_time = time.time() - start_time\n                        times.append(inference_time)\n                    \n                    avg_time = np.mean(times)\n                    std_time = np.std(times)\n                    \n                    # Memory usage (approximate)\n                    if self.device == \"cuda\":\n                        memory_used = torch.cuda.max_memory_allocated() / 1024**2  # MB\n                        torch.cuda.reset_peak_memory_stats()\n                    else:\n                        memory_used = 0  # CPU memory tracking is more complex\n                    \n                    size_results[model_name] = {\n                        \"avg_inference_time\": avg_time,\n                        \"std_inference_time\": std_time,\n                        \"memory_mb\": memory_used,\n                        \"success\": True,\n                        \"output_shape\": list(predictions.shape),\n                    }\n                    \n                    print(f\"      ‚úÖ Time: {avg_time:.4f}s ¬± {std_time:.4f}s, \"\n                          f\"Memory: {memory_used:.1f}MB\")\n                \n                except Exception as e:\n                    print(f\"      ‚ùå Error: {e}\")\n                    size_results[model_name] = {\n                        \"success\": False,\n                        \"error\": str(e),\n                    }\n            \n            scalability_results[size] = {\n                \"num_nodes\": size,\n                \"num_edges\": kg.edge_index.size(1),\n                \"models\": size_results,\n            }\n        \n        # Analysis\n        print(\"\\nüìä Scalability Analysis:\")\n        print(\"-\" * 25)\n        \n        analysis = self._analyze_scalability_results(scalability_results)\n        \n        scalability_study_results = {\n            \"results\": scalability_results,\n            \"analysis\": analysis,\n            \"graph_sizes\": graph_sizes,\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        }\n        \n        self._save_results(scalability_study_results, \"scalability_study\")\n        \n        return scalability_study_results\n    \n    # Helper methods\n    \n    def _generate_multi_domain_datasets(self, domains: List[str], num_graphs: int) -> Dict:\n        \"\"\"Generate datasets for multiple domains.\"\"\"\n        datasets = {}\n        data_gen = SyntheticDataGenerator()\n        \n        for domain in domains:\n            print(f\"   üìä Generating {domain} graphs...\")\n            graphs = data_gen.generate_domain_graphs(domain, num_graphs=num_graphs)\n            datasets[domain] = graphs\n        \n        return datasets\n    \n    def _initialize_all_models(self) -> Dict:\n        \"\"\"Initialize all models for comparison.\"\"\"\n        models = {\n            \"HyperGNN\": HyperGNN(hidden_dim=128).to(self.device),\n            \"AdaptiveHyperGNN\": DimensionAdaptiveHyperNetwork(\n                text_dim=128, max_hidden_dim=128\n            ).to(self.device),\n            \"TraditionalGAT\": TraditionalGNN(\n                input_dim=128, hidden_dim=128, output_dim=64, gnn_type=\"GAT\"\n            ).to(self.device),\n            \"TraditionalGCN\": TraditionalGNN(\n                input_dim=128, hidden_dim=128, output_dim=64, gnn_type=\"GCN\"\n            ).to(self.device),\n            \"ZeroShotText\": ZeroShotTextGNN(\n                input_dim=128, hidden_dim=128, output_dim=64\n            ).to(self.device),\n        }\n        \n        return models\n    \n    def _evaluate_model_on_datasets(self, model, model_name: str, datasets: Dict) -> Dict:\n        \"\"\"Evaluate a model on all datasets.\"\"\"\n        model_results = {}\n        \n        for domain, graphs in datasets.items():\n            print(f\"     üìä {domain}...\")\n            \n            domain_results = []\n            \n            for i, graph in enumerate(graphs[:3]):  # Test on subset\n                try:\n                    kg = TextualKnowledgeGraph.from_dict(graph)\n                    \n                    start_time = time.time()\n                    model.eval()\n                    \n                    with torch.no_grad():\n                        if \"HyperGNN\" in model_name or \"Adaptive\" in model_name:\n                            predictions = model(\n                                kg.edge_index.to(self.device),\n                                kg.node_features.to(self.device),\n                                kg.node_texts,\n                            )\n                        else:\n                            predictions = model(\n                                kg.node_features.to(self.device),\n                                kg.edge_index.to(self.device),\n                            )\n                    \n                    inference_time = time.time() - start_time\n                    \n                    # Simple evaluation metrics\n                    result = {\n                        \"inference_time\": inference_time,\n                        \"output_shape\": list(predictions.shape),\n                        \"num_nodes\": len(kg.node_texts),\n                        \"num_edges\": kg.edge_index.size(1),\n                        \"success\": True,\n                    }\n                    \n                except Exception as e:\n                    result = {\n                        \"success\": False,\n                        \"error\": str(e),\n                        \"inference_time\": float('inf'),\n                    }\n                \n                domain_results.append(result)\n            \n            # Aggregate domain results\n            successful_results = [r for r in domain_results if r[\"success\"]]\n            \n            if successful_results:\n                avg_time = np.mean([r[\"inference_time\"] for r in successful_results])\n                success_rate = len(successful_results) / len(domain_results)\n            else:\n                avg_time = float('inf')\n                success_rate = 0.0\n            \n            model_results[domain] = {\n                \"avg_inference_time\": avg_time,\n                \"success_rate\": success_rate,\n                \"detailed_results\": domain_results,\n            }\n        \n        return model_results\n    \n    def _perform_statistical_analysis(self, results: Dict, alpha: float) -> Dict:\n        \"\"\"Perform statistical analysis on results.\"\"\"\n        analysis = {}\n        \n        # Extract inference times for each model across all runs\n        model_times = {}\n        \n        for run_key, run_data in results.items():\n            for model_name, model_data in run_data.items():\n                if model_name not in model_times:\n                    model_times[model_name] = []\n                \n                # Collect all successful inference times\n                for domain_data in model_data.values():\n                    if domain_data[\"success_rate\"] > 0:\n                        model_times[model_name].append(domain_data[\"avg_inference_time\"])\n        \n        # Statistical tests\n        model_names = list(model_times.keys())\n        pairwise_tests = {}\n        \n        for i, model1 in enumerate(model_names):\n            for model2 in model_names[i+1:]:\n                if model1 in model_times and model2 in model_times:\n                    times1 = model_times[model1]\n                    times2 = model_times[model2]\n                    \n                    if len(times1) > 1 and len(times2) > 1:\n                        # Welch's t-test (unequal variances)\n                        t_stat, p_value = stats.ttest_ind(\n                            times1, times2, equal_var=False\n                        )\n                        \n                        pairwise_tests[f\"{model1}_vs_{model2}\"] = {\n                            \"t_statistic\": t_stat,\n                            \"p_value\": p_value,\n                            \"significant\": p_value < alpha,\n                            \"effect_size\": (np.mean(times1) - np.mean(times2)) / \n                                          np.sqrt((np.var(times1) + np.var(times2)) / 2),\n                        }\n        \n        # Model rankings\n        model_performance = {}\n        for model_name, times in model_times.items():\n            if times:\n                model_performance[model_name] = {\n                    \"mean_time\": np.mean(times),\n                    \"std_time\": np.std(times),\n                    \"median_time\": np.median(times),\n                    \"num_measurements\": len(times),\n                }\n        \n        # Rank by mean performance\n        ranking = sorted(\n            model_performance.items(),\n            key=lambda x: x[1][\"mean_time\"]\n        )\n        \n        analysis = {\n            \"pairwise_tests\": pairwise_tests,\n            \"model_performance\": model_performance,\n            \"ranking\": [(name, perf[\"mean_time\"]) for name, perf in ranking],\n            \"significance_level\": alpha,\n            \"summary\": self._generate_statistical_summary(pairwise_tests, ranking),\n        }\n        \n        return analysis\n    \n    def _generate_statistical_summary(self, tests: Dict, ranking: List) -> str:\n        \"\"\"Generate human-readable statistical summary.\"\"\"\n        summary = []\n        summary.append(f\"üìä Statistical Analysis Summary:\")\n        summary.append(f\"   üèÜ Best performing model: {ranking[0][0]} ({ranking[0][1]:.4f}s)\")\n        \n        if len(ranking) > 1:\n            summary.append(f\"   ü•à Second best: {ranking[1][0]} ({ranking[1][1]:.4f}s)\")\n        \n        significant_tests = [name for name, test in tests.items() if test[\"significant\"]]\n        summary.append(f\"   üî¨ Significant differences found: {len(significant_tests)}/{len(tests)}\")\n        \n        return \"\\n\".join(summary)\n    \n    def _save_results(self, results: Dict, experiment_name: str):\n        \"\"\"Save results to JSON file.\"\"\"\n        os.makedirs(\"benchmark_results\", exist_ok=True)\n        \n        filename = f\"benchmark_results/{experiment_name}_{int(time.time())}.json\"\n        \n        # Convert numpy types to Python types for JSON serialization\n        json_results = self._convert_to_json_serializable(results)\n        \n        with open(filename, 'w') as f:\n            json.dump(json_results, f, indent=2)\n        \n        print(f\"   üíæ Results saved to {filename}\")\n    \n    def _convert_to_json_serializable(self, obj):\n        \"\"\"Convert numpy types to JSON serializable types.\"\"\"\n        if isinstance(obj, dict):\n            return {key: self._convert_to_json_serializable(value) for key, value in obj.items()}\n        elif isinstance(obj, list):\n            return [self._convert_to_json_serializable(item) for item in obj]\n        elif isinstance(obj, (np.integer, np.floating)):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return obj\n    \n    def _generate_visualizations(self, results: Dict):\n        \"\"\"Generate visualization plots for results.\"\"\"\n        try:\n            plt.style.use('seaborn-v0_8')\n        except:\n            plt.style.use('default')\n        \n        # Performance comparison plot\n        if \"statistical_analysis\" in results:\n            model_perf = results[\"statistical_analysis\"][\"model_performance\"]\n            \n            if model_perf:\n                models = list(model_perf.keys())\n                mean_times = [model_perf[model][\"mean_time\"] for model in models]\n                std_times = [model_perf[model][\"std_time\"] for model in models]\n                \n                plt.figure(figsize=(12, 6))\n                plt.bar(models, mean_times, yerr=std_times, capsize=5, alpha=0.7)\n                plt.xlabel('Model')\n                plt.ylabel('Average Inference Time (s)')\n                plt.title('Model Performance Comparison')\n                plt.xticks(rotation=45)\n                plt.tight_layout()\n                \n                os.makedirs(\"benchmark_results/plots\", exist_ok=True)\n                plt.savefig(\"benchmark_results/plots/performance_comparison.png\", dpi=300)\n                plt.close()\n                \n                print(\"   üìä Performance comparison plot saved\")\n    \n    # Additional helper methods for specific experiments\n    \n    def _test_hypergnn_transfer(self, model, source_graphs: List, target_graphs: List) -> float:\n        \"\"\"Test HyperGNN zero-shot transfer.\"\"\"\n        # Simple evaluation: test if model can process target domain graphs\n        correct = 0\n        total = 0\n        \n        for graph in target_graphs:\n            try:\n                kg = TextualKnowledgeGraph.from_dict(graph)\n                \n                with torch.no_grad():\n                    predictions = model(\n                        kg.edge_index.to(self.device),\n                        kg.node_features.to(self.device),\n                        kg.node_texts,\n                    )\n                \n                # Simple success metric: valid output shape\n                if predictions.shape[0] == len(kg.node_texts):\n                    correct += 1\n                total += 1\n                \n            except Exception:\n                total += 1\n        \n        return correct / total if total > 0 else 0.0\n    \n    def _test_text_similarity_transfer(self, model, source_graphs: List, target_graphs: List) -> float:\n        \"\"\"Test text similarity based transfer.\"\"\"\n        # Add source examples to memory\n        for graph in source_graphs:\n            kg = TextualKnowledgeGraph.from_dict(graph)\n            # Create dummy labels for memory bank\n            dummy_labels = torch.randint(0, 10, (len(kg.node_texts),))\n            model.add_to_memory(kg.node_texts, kg.node_features, dummy_labels)\n        \n        # Test on target graphs\n        correct = 0\n        total = 0\n        \n        for graph in target_graphs:\n            try:\n                kg = TextualKnowledgeGraph.from_dict(graph)\n                \n                with torch.no_grad():\n                    predictions = model.predict_zero_shot(\n                        kg.node_texts,\n                        kg.node_features.to(self.device),\n                        kg.edge_index.to(self.device),\n                    )\n                \n                if predictions.shape[0] == len(kg.node_texts):\n                    correct += 1\n                total += 1\n                \n            except Exception:\n                total += 1\n        \n        return correct / total if total > 0 else 0.0\n    \n    def _test_maml_transfer(self, model, source_graphs: List, target_graphs: List) -> float:\n        \"\"\"Test MAML-based transfer.\"\"\"\n        # Prepare support data\n        support_data = []\n        \n        for graph in source_graphs:\n            kg = TextualKnowledgeGraph.from_dict(graph)\n            dummy_labels = torch.randint(0, 10, (len(kg.node_texts),))\n            support_data.append((\n                kg.node_features.to(self.device),\n                kg.edge_index.to(self.device),\n                dummy_labels.to(self.device),\n            ))\n        \n        # Adapt model\n        try:\n            adapted_model = model.adapt(support_data)\n        except Exception:\n            return 0.0\n        \n        # Test on target graphs\n        correct = 0\n        total = 0\n        \n        for graph in target_graphs:\n            try:\n                kg = TextualKnowledgeGraph.from_dict(graph)\n                \n                with torch.no_grad():\n                    predictions = adapted_model(\n                        kg.node_features.to(self.device),\n                        kg.edge_index.to(self.device),\n                    )\n                \n                if predictions.shape[0] == len(kg.node_texts):\n                    correct += 1\n                total += 1\n                \n            except Exception:\n                total += 1\n        \n        return correct / total if total > 0 else 0.0\n    \n    def _analyze_ablation_results(self, results: Dict) -> Dict:\n        \"\"\"Analyze ablation study results.\"\"\"\n        analysis = {}\n        \n        # Find best configuration\n        best_config = max(\n            results.items(),\n            key=lambda x: (x[1][\"success_rate\"], -x[1][\"avg_inference_time\"])\n        )\n        \n        analysis[\"best_configuration\"] = {\n            \"name\": best_config[0],\n            \"performance\": best_config[1],\n        }\n        \n        # Component importance analysis\n        component_effects = {}\n        \n        # Analyze attention effect\n        attention_configs = {k: v for k, v in results.items() if \"attention\" in k.lower()}\n        no_attention_configs = {k: v for k, v in results.items() if \"no_attention\" in k.lower()}\n        \n        if attention_configs and no_attention_configs:\n            att_avg = np.mean([v[\"avg_inference_time\"] for v in attention_configs.values()])\n            no_att_avg = np.mean([v[\"avg_inference_time\"] for v in no_attention_configs.values()])\n            component_effects[\"attention\"] = {\n                \"time_difference\": att_avg - no_att_avg,\n                \"relative_improvement\": (no_att_avg - att_avg) / no_att_avg if no_att_avg > 0 else 0,\n            }\n        \n        analysis[\"component_effects\"] = component_effects\n        \n        return analysis\n    \n    def _analyze_transfer_results(self, results: Dict) -> Dict:\n        \"\"\"Analyze zero-shot transfer results.\"\"\"\n        analysis = {}\n        \n        # Domain difficulty analysis\n        domain_performance = {}\n        \n        # Extract all domain pairs\n        for pair_name, pair_results in results.items():\n            source_domain, target_domain = pair_name.split(\"_to_\")\n            \n            if target_domain not in domain_performance:\n                domain_performance[target_domain] = []\n            \n            # Get HyperGNN performance for this transfer\n            if \"HyperGNN\" in pair_results and pair_results[\"HyperGNN\"][\"success\"]:\n                domain_performance[target_domain].append(pair_results[\"HyperGNN\"][\"accuracy\"])\n        \n        # Compute domain difficulty scores\n        domain_difficulty = {}\n        for domain, accuracies in domain_performance.items():\n            if accuracies:\n                avg_acc = np.mean(accuracies)\n                domain_difficulty[domain] = {\n                    \"average_transfer_accuracy\": avg_acc,\n                    \"difficulty_score\": 1 - avg_acc,  # Higher = more difficult\n                    \"num_transfers\": len(accuracies),\n                }\n        \n        # Rank domains by difficulty\n        difficulty_ranking = sorted(\n            domain_difficulty.items(),\n            key=lambda x: x[1][\"difficulty_score\"],\n            reverse=True\n        )\n        \n        analysis[\"domain_difficulty\"] = domain_difficulty\n        analysis[\"difficulty_ranking\"] = difficulty_ranking\n        \n        return analysis\n    \n    def _analyze_scalability_results(self, results: Dict) -> Dict:\n        \"\"\"Analyze scalability study results.\"\"\"\n        analysis = {}\n        \n        # Extract scaling trends for each model\n        model_scaling = {}\n        \n        for size, size_data in results.items():\n            for model_name, model_data in size_data[\"models\"].items():\n                if model_data[\"success\"]:\n                    if model_name not in model_scaling:\n                        model_scaling[model_name] = {\n                            \"sizes\": [],\n                            \"times\": [],\n                            \"memory\": [],\n                        }\n                    \n                    model_scaling[model_name][\"sizes\"].append(size)\n                    model_scaling[model_name][\"times\"].append(model_data[\"avg_inference_time\"])\n                    model_scaling[model_name][\"memory\"].append(model_data.get(\"memory_mb\", 0))\n        \n        # Compute scaling coefficients (linear fit)\n        scaling_analysis = {}\n        \n        for model_name, data in model_scaling.items():\n            if len(data[\"sizes\"]) > 2:\n                # Linear regression for time complexity\n                time_slope, time_intercept = np.polyfit(data[\"sizes\"], data[\"times\"], 1)\n                \n                # Linear regression for memory usage\n                if any(m > 0 for m in data[\"memory\"]):\n                    memory_slope, memory_intercept = np.polyfit(data[\"sizes\"], data[\"memory\"], 1)\n                else:\n                    memory_slope, memory_intercept = 0, 0\n                \n                scaling_analysis[model_name] = {\n                    \"time_complexity\": {\n                        \"slope\": time_slope,\n                        \"intercept\": time_intercept,\n                        \"scaling_factor\": time_slope,  # Time per node\n                    },\n                    \"memory_complexity\": {\n                        \"slope\": memory_slope,\n                        \"intercept\": memory_intercept,\n                        \"scaling_factor\": memory_slope,  # Memory per node\n                    },\n                    \"efficiency_score\": 1 / (1 + time_slope),  # Higher = more efficient\n                }\n        \n        # Rank models by efficiency\n        efficiency_ranking = sorted(\n            scaling_analysis.items(),\n            key=lambda x: x[1][\"efficiency_score\"],\n            reverse=True\n        )\n        \n        analysis[\"model_scaling\"] = model_scaling\n        analysis[\"scaling_analysis\"] = scaling_analysis\n        analysis[\"efficiency_ranking\"] = efficiency_ranking\n        \n        return analysis\n\n\ndef main():\n    \"\"\"Run comprehensive research experiments.\"\"\"\n    print(\"üî¨ Graph Hypernetwork Forge - Research Experiment Suite\")\n    print(\"=\" * 60)\n    \n    # Initialize experiment suite\n    suite = ResearchExperimentSuite()\n    \n    # Run all experiments\n    print(\"\\nüöÄ Starting Comprehensive Research Study...\")\n    \n    # 1. Comprehensive Benchmark\n    print(\"\\n\" + \"=\" * 60)\n    benchmark_results = suite.run_comprehensive_benchmark(\n        domains=[\"social\", \"citation\", \"biological\"],\n        num_graphs_per_domain=3,\n        num_runs=2,\n    )\n    \n    # 2. Ablation Study\n    print(\"\\n\" + \"=\" * 60)\n    ablation_results = suite.run_ablation_study()\n    \n    # 3. Zero-Shot Transfer Study\n    print(\"\\n\" + \"=\" * 60)\n    transfer_results = suite.run_zero_shot_transfer_study()\n    \n    # 4. Scalability Study\n    print(\"\\n\" + \"=\" * 60)\n    scalability_results = suite.run_scalability_study()\n    \n    # Generate final research report\n    print(\"\\n\" + \"=\" * 60)\n    print(\"üìä RESEARCH STUDY COMPLETE\")\n    print(\"=\" * 60)\n    \n    print(\"\\nüèÜ Key Findings:\")\n    \n    # Best performing model\n    if benchmark_results.get(\"statistical_analysis\"):\n        ranking = benchmark_results[\"statistical_analysis\"][\"ranking\"]\n        if ranking:\n            print(f\"   ü•á Best Overall Model: {ranking[0][0]} ({ranking[0][1]:.4f}s avg)\")\n    \n    # Best ablation configuration\n    if ablation_results.get(\"ranking\"):\n        best_config = ablation_results[\"ranking\"][0]\n        print(f\"   ‚öôÔ∏è Best Configuration: {best_config}\")\n    \n    # Transfer learning insights\n    if transfer_results.get(\"model_averages\"):\n        hypergnn_perf = transfer_results[\"model_averages\"].get(\"HyperGNN\")\n        if hypergnn_perf:\n            print(f\"   üéØ HyperGNN Transfer Accuracy: {hypergnn_perf['mean_accuracy']:.3f} ¬± {hypergnn_perf['std_accuracy']:.3f}\")\n    \n    # Scalability insights\n    if scalability_results.get(\"analysis\", {}).get(\"efficiency_ranking\"):\n        most_efficient = scalability_results[\"analysis\"][\"efficiency_ranking\"][0]\n        print(f\"   üìà Most Scalable Model: {most_efficient[0]} (efficiency: {most_efficient[1]['efficiency_score']:.3f})\")\n    \n    print(\"\\nüìÅ All results saved to benchmark_results/ directory\")\n    print(\"üéâ Research experiments completed successfully!\")\n\n\nif __name__ == \"__main__\":\n    main()