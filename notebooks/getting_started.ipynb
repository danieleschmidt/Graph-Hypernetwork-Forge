{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Hypernetwork Forge - Getting Started\n",
    "\n",
    "This notebook demonstrates the core capabilities of Graph Hypernetwork Forge, a framework for generating GNN weights dynamically from textual node descriptions.\n",
    "\n",
    "## Key Features\n",
    "- **Dynamic Weight Generation**: GNN parameters generated from text\n",
    "- **Zero-Shot Transfer**: Apply to unseen knowledge graph domains\n",
    "- **Modular Architecture**: Swap GNN backends and text encoders\n",
    "- **Production Ready**: Training, evaluation, and deployment tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install graph-hypernetwork-forge\n",
    "```\n",
    "\n",
    "Or for development:\n",
    "```bash\n",
    "git clone https://github.com/danieleschmidt/Graph-Hypernetwork-Forge.git\n",
    "cd Graph-Hypernetwork-Forge\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from graph_hypernetwork_forge import HyperGNN, TextualKnowledgeGraph\n",
    "from graph_hypernetwork_forge.utils import SyntheticDataGenerator, HyperGNNTrainer\n",
    "\n",
    "print(\"Graph Hypernetwork Forge loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Knowledge Graphs with Textual Metadata\n",
    "\n",
    "Let's start by creating a knowledge graph where each node has textual descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple knowledge graph\n",
    "edge_index = torch.tensor([\n",
    "    [0, 1, 1, 2, 2, 3, 3, 0],\n",
    "    [1, 0, 2, 1, 3, 2, 0, 3]\n",
    "], dtype=torch.long)\n",
    "\n",
    "node_texts = [\n",
    "    \"Alice is a machine learning researcher specializing in neural networks.\",\n",
    "    \"Bob works as a software engineer developing AI applications.\", \n",
    "    \"Carol is a data scientist focusing on natural language processing.\",\n",
    "    \"David is a product manager for AI-powered tools.\"\n",
    "]\n",
    "\n",
    "# Optional: add node features\n",
    "node_features = torch.randn(4, 16)\n",
    "node_labels = torch.tensor([0, 1, 0, 1])  # 0: Technical, 1: Management\n",
    "\n",
    "# Create the knowledge graph\n",
    "kg = TextualKnowledgeGraph(\n",
    "    edge_index=edge_index,\n",
    "    node_texts=node_texts,\n",
    "    node_features=node_features,\n",
    "    node_labels=node_labels,\n",
    "    metadata={\"domain\": \"professional_network\"}\n",
    ")\n",
    "\n",
    "print(f\"Created knowledge graph with {kg.num_nodes} nodes and {kg.num_edges} edges\")\n",
    "print(f\"Domain: {kg.metadata['domain']}\")\n",
    "\n",
    "# Display graph statistics\n",
    "stats = kg.statistics()\n",
    "for key, value in stats.items():\n",
    "    if key != \"metadata\":\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HyperGNN Model Initialization\n",
    "\n",
    "The HyperGNN model consists of three main components:\n",
    "1. **Text Encoder**: Converts text to embeddings\n",
    "2. **Hypernetwork**: Generates GNN weights from text embeddings\n",
    "3. **Dynamic GNN**: Applies generated weights to graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HyperGNN model\n",
    "model = HyperGNN(\n",
    "    text_encoder=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    gnn_backbone=\"GAT\",  # Can be GCN, GAT, or SAGE\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"Model Architecture:\")\n",
    "print(f\"  Text Encoder: {model.text_encoder_name}\")\n",
    "print(f\"  GNN Backbone: {model.gnn_backbone}\")\n",
    "print(f\"  Hidden Dimension: {model.hidden_dim}\")\n",
    "print(f\"  Number of Layers: {model.num_layers}\")\n",
    "\n",
    "# Count model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"  Total Parameters: {total_params:,}\")\n",
    "print(f\"  Trainable Parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dynamic Weight Generation\n",
    "\n",
    "The key innovation is generating GNN weights dynamically from textual descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate weights from node texts\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    weights = model.generate_weights(kg.node_texts)\n",
    "\n",
    "print(f\"Generated weights for {len(weights)} layers:\")\n",
    "for layer_idx, layer_weights in enumerate(weights):\n",
    "    print(f\"\\nLayer {layer_idx}:\")\n",
    "    for weight_name, weight_tensor in layer_weights.items():\n",
    "        print(f\"  {weight_name}: {list(weight_tensor.shape)}\")\n",
    "\n",
    "# Visualize weight variation across different text descriptions\n",
    "first_layer_weights = weights[0][\"weight\"]  # [num_nodes, in_dim, out_dim]\n",
    "weight_norms = torch.norm(first_layer_weights, dim=(1, 2))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(len(kg.node_texts)), weight_norms.numpy())\n",
    "plt.xlabel('Node Index')\n",
    "plt.ylabel('Weight Norm')\n",
    "plt.title('Weight Variation Across Different Text Descriptions')\n",
    "plt.xticks(range(len(kg.node_texts)), [f'Node {i}' for i in range(len(kg.node_texts))])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWeight norms for each node:\")\n",
    "for i, norm in enumerate(weight_norms):\n",
    "    print(f\"  Node {i} ('{kg.node_texts[i][:30]}...'): {norm.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forward Pass and Predictions\n",
    "\n",
    "Now let's perform a forward pass to get node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    node_embeddings = model(kg.edge_index, kg.node_features, kg.node_texts)\n",
    "\n",
    "print(f\"Generated node embeddings: {node_embeddings.shape}\")\n",
    "print(f\"Embedding sample (first 5 dimensions):\")\n",
    "for i, embedding in enumerate(node_embeddings[:, :5]):\n",
    "    print(f\"  Node {i}: {embedding.tolist()}\")\n",
    "\n",
    "# Compute pairwise similarities\n",
    "similarities = torch.cosine_similarity(\n",
    "    node_embeddings.unsqueeze(1), \n",
    "    node_embeddings.unsqueeze(0), \n",
    "    dim=2\n",
    ")\n",
    "\n",
    "# Visualize similarity matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(similarities.numpy(), cmap='viridis', vmin=0, vmax=1)\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.title('Node Embedding Similarities')\n",
    "plt.xlabel('Node Index')\n",
    "plt.ylabel('Node Index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPairwise similarities:\")\n",
    "for i in range(len(kg.node_texts)):\n",
    "    for j in range(i+1, len(kg.node_texts)):\n",
    "        sim = similarities[i, j].item()\n",
    "        print(f\"  Node {i} ↔ Node {j}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Zero-Shot Transfer Demonstration\n",
    "\n",
    "The key advantage of HyperGNN is its ability to work on completely new domains without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data from different domains\n",
    "generator = SyntheticDataGenerator(seed=42)\n",
    "\n",
    "# Source domain: Professional network (similar to our example)\n",
    "source_kg = generator.generate_social_network(num_nodes=6, num_classes=2)\n",
    "\n",
    "# Target domain: Academic papers (very different text patterns)\n",
    "target_kg = generator.generate_citation_network(num_nodes=6, num_classes=2)\n",
    "\n",
    "print(\"Source Domain (Social Network):\")\n",
    "for i, text in enumerate(source_kg.node_texts[:3]):\n",
    "    print(f\"  {i+1}. {text}\")\n",
    "\n",
    "print(\"\\nTarget Domain (Citation Network):\")\n",
    "for i, text in enumerate(target_kg.node_texts[:3]):\n",
    "    print(f\"  {i+1}. {text}\")\n",
    "\n",
    "# Apply the SAME model to both domains\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Source domain predictions\n",
    "    source_embeddings = model(\n",
    "        source_kg.edge_index, \n",
    "        source_kg.node_features, \n",
    "        source_kg.node_texts\n",
    "    )\n",
    "    \n",
    "    # Target domain predictions (zero-shot!)\n",
    "    target_embeddings = model(\n",
    "        target_kg.edge_index, \n",
    "        target_kg.node_features, \n",
    "        target_kg.node_texts\n",
    "    )\n",
    "\n",
    "print(f\"\\nSource embeddings shape: {source_embeddings.shape}\")\n",
    "print(f\"Target embeddings shape: {target_embeddings.shape}\")\n",
    "print(\"✅ Zero-shot transfer successful!\")\n",
    "\n",
    "# Analyze domain differences\n",
    "source_mean = source_embeddings.mean(dim=0)\n",
    "target_mean = target_embeddings.mean(dim=0)\n",
    "domain_similarity = torch.cosine_similarity(source_mean, target_mean, dim=0)\n",
    "\n",
    "print(f\"\\nDomain-level embedding similarity: {domain_similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training a Model\n",
    "\n",
    "Let's demonstrate how to train a HyperGNN model for a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "generator = SyntheticDataGenerator(seed=123)\n",
    "train_graphs = []\n",
    "for i in range(5):\n",
    "    graph = generator.generate_social_network(num_nodes=20, num_classes=3)\n",
    "    train_graphs.append(graph)\n",
    "\n",
    "val_graphs = []\n",
    "for i in range(2):\n",
    "    graph = generator.generate_social_network(num_nodes=15, num_classes=3)\n",
    "    val_graphs.append(graph)\n",
    "\n",
    "print(f\"Generated {len(train_graphs)} training graphs and {len(val_graphs)} validation graphs\")\n",
    "\n",
    "# Create a smaller model for quick training\n",
    "train_model = HyperGNN(\n",
    "    text_encoder=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    gnn_backbone=\"GAT\",\n",
    "    hidden_dim=64,\n",
    "    num_layers=2,\n",
    ")\n",
    "\n",
    "# Set up trainer\n",
    "trainer = HyperGNNTrainer(\n",
    "    model=train_model,\n",
    "    device=\"cpu\",  # Use CPU for demo\n",
    ")\n",
    "\n",
    "# Train for a few epochs (demo purposes)\n",
    "print(\"\\nStarting training...\")\n",
    "history = trainer.train(\n",
    "    train_graphs=train_graphs,\n",
    "    val_graphs=val_graphs,\n",
    "    num_epochs=3,  # Short training for demo\n",
    "    task_type=\"node_classification\",\n",
    "    early_stopping_patience=10,\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "if 'val_accuracy' in history:\n",
    "    plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Features\n",
    "\n",
    "### Custom Text Encoders\n",
    "You can easily swap different text encoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different text encoders\n",
    "encoders = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"bert-base-uncased\"  # This will use transformers instead of sentence-transformers\n",
    "]\n",
    "\n",
    "sample_texts = [\n",
    "    \"A machine learning researcher working on deep learning models.\",\n",
    "    \"A software engineer developing mobile applications.\"\n",
    "]\n",
    "\n",
    "for encoder_name in encoders:\n",
    "    try:\n",
    "        model = HyperGNN(\n",
    "            text_encoder=encoder_name,\n",
    "            hidden_dim=32,\n",
    "            num_layers=1\n",
    "        )\n",
    "        \n",
    "        # Generate embeddings\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.text_encoder(sample_texts)\n",
    "        \n",
    "        print(f\"Encoder: {encoder_name}\")\n",
    "        print(f\"  Embedding shape: {embeddings.shape}\")\n",
    "        print(f\"  Embedding norm: {torch.norm(embeddings, dim=1).tolist()}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Encoder {encoder_name}: Error - {e}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Manipulation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from JSON (if you have a JSON file)\n",
    "# kg_from_json = TextualKnowledgeGraph.from_json(\"your_graph.json\")\n",
    "\n",
    "# Create subgraphs\n",
    "original_kg = generator.generate_social_network(num_nodes=10, num_classes=2)\n",
    "subgraph = original_kg.subgraph([0, 1, 2, 3, 4])  # First 5 nodes\n",
    "\n",
    "print(f\"Original graph: {original_kg.num_nodes} nodes, {original_kg.num_edges} edges\")\n",
    "print(f\"Subgraph: {subgraph.num_nodes} nodes, {subgraph.num_edges} edges\")\n",
    "\n",
    "# Get neighbor information\n",
    "center_node = 0\n",
    "neighbor_texts = original_kg.get_neighbor_texts(center_node, k_hops=1)\n",
    "\n",
    "print(f\"\\nCenter node: {original_kg.node_texts[center_node][:50]}...\")\n",
    "print(f\"Neighbors:\")\n",
    "for i, text in enumerate(neighbor_texts[:3]):  # Show first 3 neighbors\n",
    "    print(f\"  {i+1}. {text[:50]}...\")\n",
    "\n",
    "# Convert to PyTorch Geometric format\n",
    "pyg_data = original_kg.to_pyg_data()\n",
    "print(f\"\\nPyTorch Geometric Data:\")\n",
    "print(f\"  Number of nodes: {pyg_data.num_nodes}\")\n",
    "print(f\"  Edge index shape: {pyg_data.edge_index.shape}\")\n",
    "if hasattr(pyg_data, 'x'):\n",
    "    print(f\"  Node features shape: {pyg_data.x.shape}\")\n",
    "if hasattr(pyg_data, 'y'):\n",
    "    print(f\"  Node labels shape: {pyg_data.y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Tips\n",
    "\n",
    "### Batch Processing\n",
    "For large graphs, process in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a larger graph\n",
    "large_kg = generator.generate_social_network(num_nodes=100, num_classes=5)\n",
    "\n",
    "# Process in batches (simulate large-scale processing)\n",
    "batch_size = 20\n",
    "all_embeddings = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(0, large_kg.num_nodes, batch_size):\n",
    "        end_idx = min(i + batch_size, large_kg.num_nodes)\n",
    "        \n",
    "        # Create subgraph for this batch\n",
    "        batch_nodes = list(range(i, end_idx))\n",
    "        batch_kg = large_kg.subgraph(batch_nodes)\n",
    "        \n",
    "        # Process batch\n",
    "        batch_embeddings = model(\n",
    "            batch_kg.edge_index,\n",
    "            batch_kg.node_features,\n",
    "            batch_kg.node_texts\n",
    "        )\n",
    "        \n",
    "        all_embeddings.append(batch_embeddings)\n",
    "        print(f\"Processed batch {i//batch_size + 1}: nodes {i}-{end_idx-1}\")\n",
    "\n",
    "# Combine all embeddings\n",
    "final_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "print(f\"\\nFinal embeddings shape: {final_embeddings.shape}\")\n",
    "print(f\"Successfully processed {large_kg.num_nodes} nodes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated:\n",
    "\n",
    "1. **Knowledge Graph Creation**: Building graphs with textual metadata\n",
    "2. **Dynamic Weight Generation**: Creating GNN weights from text descriptions\n",
    "3. **Zero-Shot Transfer**: Applying models to new domains without retraining\n",
    "4. **Training Pipeline**: Complete training and evaluation workflow\n",
    "5. **Advanced Features**: Custom encoders, graph manipulation, and batch processing\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore the `scripts/` directory for command-line tools\n",
    "- Check out `examples/` for more specialized use cases\n",
    "- Read the documentation for API details\n",
    "- Try different GNN backbones (GCN, GraphSAGE)\n",
    "- Experiment with domain-specific text encoders\n",
    "\n",
    "Happy hypernetworking! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}